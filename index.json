[{"categories":["Cetification"],"contents":"Metrics Server  one Metrics Server per k8s cluster 從每一個k8s的node和pod中取metrics並存入memory (in-memory monitor solution) 由於將metrics存在memory中, 所以沒辦法查看historical performance metrics (沒有存入disk)  Install command git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git\rcd kubernetes-metrics-server/\rkubectl create -f .\r 接著就可以使用kubectl top pod or kubectl top node查看k8s物件狀況  Metrics如何產生?  透過kubelet  kubelet內有個subcomponent叫cAdvisor會負責retriving metrics from pods並將這些metrics expose給kubelet API    補充: kubectl create vs kubectl apply   kubectl create 命令屬於 Imperative 宣告，會明確告知 k8s 要「建立」一個資源，他不會紀錄建立資源的最後狀態，真的只有幫你建立資源而已。\n  kubectl apply 命令屬於 Declarative 選告，不用明確的告訴 k8s 要如何建立一個資源，也不用管現在叢集中有沒有這個資源，他就是很單純的幫你建立起你想要的資源而已。\n 如果目前沒有資源，k8s 還會在你建立資源時，同時幫你建立一份快照(Snapshot)，紀錄在資源的 .metadata.annotations.kubectl.kubernetes.io/last-applied-configuration 底下，你日後如果對 YAML 檔案進行更新，k8s就會去比對先前的版本與最近一次的套用版本，藉此計算出差異之處，並套用差異更新。    ","permalink":"https://kuanyuce.github.io/blog/ckamonitor/","tags":["k8s","Tool"],"title":"K8s-Monitoring"},{"categories":["Tool"],"contents":"SLF4J  Maven dependencies Testcontainers is distributed as separate JARs with a common version number:\nA core JAR file for core functionality, generic containers and docker-compose support A separate JAR file for each of the specialised modules. Each module\u0026rsquo;s documentation describes the Maven/Gradle dependency to add to your project\u0026rsquo;s build. For the core library, the latest Maven/Gradle dependency is as follows:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testcontainers\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;testcontainers\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.19.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 常見問題  SLF4J: Failed to load class \u0026ldquo;org.slf4j.impl.StaticLoggerBinder\u0026rdquo; SLF4J(W): Defaulting to no-operation (NOP) logger implementation Driver not found for mariadb database container: add relative-db jdbc dependency    \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.testcontainers\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;oracle-xe\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.19.5\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/com.oracle.database.jdbc/ojdbc11 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.database.jdbc\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ojdbc11\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;23.3.0.23.09\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;     JDBC support You can obtain a temporary database in one of two ways:\nUsing a specially modified JDBC URL: after making a very simple modification to your system\u0026rsquo;s JDBC URL string, Testcontainers will provide a disposable stand-in database that can be used without requiring modification to your application code. JUnit @Rule/@ClassRule: this mode starts a database inside a container before your tests and tears it down afterwards.\nDatabase containers launched via JDBC URL scheme As long as you have Testcontainers and the appropriate JDBC driver on your classpath, you can simply modify regular JDBC connection URLs to get a fresh containerized instance of the database each time your application starts up.\n We will use /// (host-less URIs) from now on to emphasis the unimportance of the host:port pair. From Testcontainers\u0026rsquo; perspective, jdbc:mysql:8.0.36://localhost:3306/databasename and jdbc:mysql:8.0.36:///databasename is the same URI.\n JDBC URL examples  Connection String: jdbc:tc:questdb:6.5.3:///databasename  package org.example; import com.zaxxer.hikari.HikariConfig; import com.zaxxer.hikari.HikariDataSource; import org.junit.Test; import org.testcontainers.containers.OracleContainer; import java.sql.*; public class MainTest { @Test public void testOracleWithNoSpecifiedVersion() throws SQLException { OracleContainer oracle = new OracleContainer(\u0026quot;gvenzl/oracle-xe:21-slim-faststart\u0026quot;) .withDatabaseName(\u0026quot;testDB\u0026quot;) .withUsername(\u0026quot;testUser\u0026quot;) .withPassword(\u0026quot;testPassword\u0026quot;); oracle.start(); performSimpleTest(\u0026quot;jdbc:tc:oracle:21-slim-faststart:///testDB\u0026quot;); } private void performSimpleTest(String jdbcUrl) throws SQLException { HikariDataSource ds = getDataSource(jdbcUrl, 1); //Connection conn = DriverManager.getConnection(jdbcUrl, \u0026quot;testDB\u0026quot;, \u0026quot;testPassword\u0026quot;) try(Connection conn = ds.getConnection()){ PreparedStatement table = conn.prepareStatement(\u0026quot;CREATE TABLE testTest (name varchar(50),\u0026quot; + \u0026quot;age int)\u0026quot;); PreparedStatement insert = conn.prepareStatement(\u0026quot;INSERT into testTest VALUES ('age', 5)\u0026quot;); PreparedStatement stmt = conn.prepareStatement(\u0026quot;SELECT * FROM testTest\u0026quot;); table.executeUpdate(); insert.executeUpdate(); ResultSet rs = stmt.executeQuery(); while (rs.next()){ System.out.println(rs.getString(\u0026quot;name\u0026quot;) + rs.getInt(\u0026quot;age\u0026quot;)); } } ds.close(); } private HikariDataSource getDataSource(String jdbcUrl, int poolSize) { HikariConfig hikariConfig = new HikariConfig(); hikariConfig.setJdbcUrl(jdbcUrl); hikariConfig.setUsername(\u0026quot;testDB\u0026quot;); hikariConfig.setPassword(\u0026quot;testPassword\u0026quot;); return new HikariDataSource(hikariConfig); } } ","permalink":"https://kuanyuce.github.io/blog/oraclecontainer/","tags":["Java"],"title":"Oracle TestContainer"},{"categories":["Programming Language"],"contents":"靜態方法 Static method 要宣告靜態方法的泛型時, 記得要在static後面加上類別型態\n原因是因為泛型這機制其實是runtime的時候動態的幫你轉換型別\n所以實際上你在宣告(使用)的時候, 會先寫好型別,\n接著在new object的時候就會根據你宣告的型別做轉換\n但問題是靜態方法是在編譯期轉換的, 此時compiler並不知道實際的類別是什麼(不知道怎麼轉換)\n因此才需要在static後面接上型別告訴compiler編譯期要怎麼轉換\n 如果想在class的靜態方法裡上用泛型, 如何在invoke的時候, 傳入泛型類型呢? 靜態方法在 static 關鍵字之後, 宣告泛型: public static \u0026lt;R\u0026gt; Builder\u0026lt;R\u0026gt; builder() {...}\n 泛型的來源、原理  Java 會在編譯期擦除(type erasure) 所有的泛型訊息，這樣就可以讓 JVM 使用相同的bytecode而不必新增，在 JVM Runtime 時就不存在所謂的泛型訊息  泛型副作用  不能使用基本數據類型 (byte、char、short、int、long、double)，因為擦除後會轉換為 Object 類型 不能使用 instanceof 運算符號，同樣是因為類型擦除後所帶來的副作用，會導致無法判別 擦拭後都是 Object 類型，沒得判斷 不能使用在靜態方法上，因為泛型創建對象時才能確定型別，但靜態方法、參數，是不用加載就可以使用 (泛型方法則可以，因為泛型方法是呼叫後才加載)  // 假如我们定義一个泛型類Generic class Generic\u0026lt;T\u0026gt; { private T obj; public Generic(T o) { obj = o; } public T getObj() { return obj; } } //那麼Java編譯後的bytecode的Generic相當於 class Generic { private Object obj; public Generic(Object o) { obj = o; } public Object getObj() { return obj; } } Java - 偽泛型   Java / kotlin 虛擬機其實內部並無泛型，並不是真正的泛型 ，JDK 5 才有泛型，為了向上兼容，才會演變成 偽泛型\n 在編譯後的字節碼中替換成原生類型(Raw Type)，並在相應的地方插入強制轉型代碼，所以對於運行期的 Java 語言來說 Listand List就是同一類別，Java 語言中的泛型實現方法稱為 類型擦除，基於這種方法實現的泛型就是偽泛型\n   在泛型中並不是所有的擦除都會變成 Object 的，當泛型有限制性，Test，則會被擦除成 Apple\n  List list = new ArrayList\u0026lt;Integer\\\u0026gt;(); List\u0026lt;String\u0026gt; list2 = list;  擦除過後 ArrayList轉為 Array List，List轉為 List\n 參考\n","permalink":"https://kuanyuce.github.io/blog/javageneric/","tags":["Java"],"title":"Java Generic"},{"categories":["Programming Language"],"contents":"Java好用方法   Method Reference (Java8)\n 使用: 以雙冒號double colon(::)呈現 目的: 避免重複撰寫Lambda運算式, 同時能讓程式碼更為清楚 用法:   Arrays.sort(names, (name1, name2)-\u0026gt;StringOrder.byLength(name1, name2)); // 透過Method Reference改為 Arrays.sort(names, StringOrder::byLength);       靜態方法of() (Java9)\n 可以簡單看成建立List, Map, Set實例的便捷方法   Java9之後List/Set/Map接口裡面增加了一個靜態的方法of，可以給集合一次性地添加多個元素\n  使用時機： 當集合中儲存的元素個數已經確定了，不再改變時使用。 注意：  of方法只適用於List/Set/Map接口，不適用於接口的實現類 of方法的返回值是一個不能改變的集合，集合不能再使用add,put方法添加元素，否則會拋出異常 Set/Map接口在調用of方法的時候，不能有重覆的元素，否則會拋出異常   用法: static \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; of (E...elements)    text block (Java15)\n 原本text在java會以 + 和 \\n 呈現, 但是在java15後可以用text block的方式呈現 用法: String json = \u0026quot;{\\n\u0026quot; + \u0026quot; \\\u0026quot;array\\\u0026quot;: [\\n\u0026quot; + \u0026quot; 1,\\n\u0026quot; + \u0026quot; 2,\\n\u0026quot; + \u0026quot; 3\\n\u0026quot; + \u0026quot; ],\\n\u0026quot; + \u0026quot; \\\u0026quot;boolean\\\u0026quot;: true,\\n\u0026quot; + \u0026quot; \\\u0026quot;color\\\u0026quot;: \\\u0026quot;gold\\\u0026quot;,\\n\u0026quot; + \u0026quot; \\\u0026quot;null\\\u0026quot;: null,\\n\u0026quot; + \u0026quot; \\\u0026quot;number\\\u0026quot;: 123,\\n\u0026quot; + \u0026quot; \\\u0026quot;object\\\u0026quot;: {\\n\u0026quot; + \u0026quot; \\\u0026quot;a\\\u0026quot;: \\\u0026quot;b\\\u0026quot;,\\n\u0026quot; + \u0026quot; \\\u0026quot;c\\\u0026quot;: \\\u0026quot;d\\\u0026quot;\\n\u0026quot; + \u0026quot; },\\n\u0026quot; + \u0026quot; \\\u0026quot;string\\\u0026quot;: \\\u0026quot;Hello World\\\u0026quot;\\n\u0026quot; + \u0026quot;}\u0026quot;; String json = \u0026quot;\u0026quot;\u0026quot; { \u0026quot;b1oolean\u0026quot;: true, \u0026quot;color\u0026quot;: \u0026quot;gold\u0026quot;, \u0026quot;number\u0026quot;: 123, \u0026quot;string\u0026quot;: \u0026quot;Hello World\u0026quot; } \u0026quot;\u0026quot;\u0026quot;;     Data type lombok vs record vs pojo POJO/Lombok/Record的用法可參考此篇\n值得注意的是record定義的資料載體可以透過\u0026lt;record\u0026gt;::\u0026lt;attribute\u0026gt;去拿到該record的值 (例如下面範例的Employee::age)\n通常會和of()以及stream()一起使用\npublic class Demo { public static void main(String[] args) { var employees = List.of( new Employee(\u0026quot;Justin\u0026quot;, 39, Gender.MALE), new Employee(\u0026quot;Monica\u0026quot;, 36, Gender.FEMALE), new Employee(\u0026quot;Irene\u0026quot;, 6, Gender.FEMALE) ); var sum = employees.stream() .filter(employee -\u0026gt; employee.gender() == Gender.MALE) .mapToInt(Employee::age) .sum(); var average = employees.stream() .filter(employee -\u0026gt; employee.gender() == Gender.MALE) .mapToInt(Employee::age) .average() .getAsDouble(); var max = employees.stream() .filter(employee -\u0026gt; employee.gender() == Gender.MALE) .mapToInt(Employee::age) .max() .getAsInt(); List.of(sum, average, max).forEach(out::println); } enum Gender { FEMALE, MALE } record Employee(String name, Integer age, Gender gender) {} } map vs mapToInt 兩者返回的數據類型不同\n  map返回Stream object\n 就只是返回一個通用的stream類型, 可以繼續接map or reduce or filter..等 Stream\u0026lt;T\u0026gt;方法    mapToInt返回IntStream\n IntStream提供了sum(), average(), max(), min()等方法    Record 大致說來，Kotlin中叫data class，Java中叫Record Class這個名稱不太一樣以外，其它的都是極為類似的。\n我們用Java17中的Record Class 來重寫上述這個類，代碼是這樣的：\npublic record EmployeeDTO(String name,String idCard,int age){} 是不是幾乎和Kotlin中的data class一模一樣呢?\n是的，就是這麽回事，它簡化了數據類的定義。所以如果你非常厭煩Java數據類的重覆定義，與其去使用lombok這種侵入性非常強的第三方庫，還不如升級使用Java 17。更詳細的介紹可參考此篇\nRecord class需要關注的點 關於record class，仍然有一些基本原則你需要知道。\n 不能在record類的body中添加屬性，屬性只能定義在類的括號後面（稱為header）  public record EmployeeDTO(String name,String idCard,int age){ //這是不允許的 private String description; } 可以在record類中添加靜態屬性與方法  public record EmployeeDTO(String name,String idCard,int age){ //這是允許的 private static System.Logger logger = System.getLogger(EmployeeDTO.class.getName()); } 可以添加額外的類方法，這是允許的  public record EmployeeDTO(String name,String idCard,int age){ //這是允許的 public String toJson(){ //... return \u0026quot;\u0026quot;; } } 比如，你可以添加一個方法，有時候我們需要將數據對象轉換為JSON來傳輸或存儲，那就添加一個toJson方法就好了.\n可以覆蓋默認生成的一些東西  public record EmployeeDTO(String name,String idCard,int age){ //這是允許的 public int getAge(){ return age; } } 可以在方法中定義Local Record Classes  在方法內部，你可以定義一個局部本地的record類\npublic void calculateLocation(double x,double y){ //定義一個本地record類 record Point(double x, double y) {} var point = new Point(x,y); //... } 這個在一些局部方法中需要封裝一些參數時，又沒必要把這個類定義在外面時非常有用。\nJava中的所有record類，默認都實現了Record接口  @Test void testRecord(){ record Point(double x,double y){} var point = new Point(0,0); // point實現了Record接口的 Assertions.assertTrue(point instanceof Record); } String Java的字串處理起來相對麻煩, 因為java不支援透過 [] 直接存取字串內的字元, 而且無法修改!\n一定要轉換成char[]類型後才能修改\nString s1 = \u0026quot;This is a sample!\u0026quot;; char c = s1.charAt(8); char[] chars = s1.toCharArray(); chars[8] = 'n'; String s2 = new String(chars); System.out.println(s2); // 輸出This is n sample! 不能直接修改Java的字串, 要先用toCharArray轉換成Char[]類型的array才行, 改完後還要再轉換回String類型\n另外, 雖然字串支援用 + 來串接字元, 但是不建議在for loop內使用, 可以改成用StringBuilder\nStringBuilder sb = new StringBuilder(); for (char c = 'a'; c \u0026lt; 'f'; c++) { sb.append(c); } String result = sb.toString(); System.out.println(result); 另外注意的是java會用單引號或雙引號來判斷文字是char or string的資料型態. (\u0026lsquo;c'是char, \u0026ldquo;c\u0026quot;是String)\n","permalink":"https://kuanyuce.github.io/blog/javafeature/","tags":["Java"],"title":"Java好用的feature"},{"categories":["Database"],"contents":"啟動MySQL伺服器的方式  mysqld mysqld_safe mysql.server mysqld_multi  檢查Char set 編碼: 字元(看得懂的字) -\u0026gt; 二進位資料 解碼: 二進位資料 -\u0026gt; 字元 一般來說，用戶端encode request string的時候使用的charset和OS當前用的charset會一致\n# 優先級: LC_ALL \u0026gt; LC_CTYPE \u0026gt; LANG # 呼叫的系統函數為nl_langinfo(CODESET) echo $LC_ALL echo $LC_CTYPE echo $LANG # windows中可以在terminal輸入chcp查到字碼頁 (內碼表) # windows把字元集稱為code page，翻譯成字碼頁或內碼表 # 呼叫的系統函數為GetConsoleCP # 可以用default-character-set指定編碼，(linux不適用) server會使用character-set-client這個變數指定的charset進行編碼\n   系統變數 說明     character-set-client Server認為request是按照這個變數指定的charset進行編碼   character-set-connection Server在處理request時，會把請求的byte string(sequence?) 從character-set-client 轉為character-set-connection   character-set-results Server採用這個變數指定的charset對回傳給client端的字串進行編碼    上面三個變數的作用範圍都是SESSION級\ncharacter-set-client vs character-set-connection 為什麼需要character-set-connection? 考慮一種情況 select 'a' = 'A' 這種情況下因為不知道這兩個字串是採用什麼字元集編碼，也不知道比較的規則是什麼 這時候character-set-connection就發揮作用了，他不管client端用什麼字元集規則，反正我統一轉換成character-set-connection設定的字元集去編碼，同時還有一個配套的系統變數collation_connetcion，這樣我們就可以知道比較這些字串該用哪些規則\n單表查詢 有的查詢可用index merge的方式利用多個Index完成查詢，具體方式有三種:\n intersection index merge union index merge sort-union index merge  Join Join過程 以下列SQL為例:\nSELECT * FROM t1, t2 WHERE t1.m1\u0026gt;1 AND t1.m1 = t2.m2 AND t2.n2 \u0026lt; 'd'  確定第一章要查詢的表，稱為驅動表 從驅動表每獲取一筆紀錄，都需要到t2表中尋找匹配的資料  :::info 並不是將所有滿足條件的驅動表資料先查詢出來放到一個地方，然後在去被驅動表中查詢，而是每獲得一筆驅動表資料，就立即到被驅動表中尋找匹配的紀錄 :::\n INNER JOIN: 對於inner join的兩張表，若驅動表中的紀錄在被驅動表中找不到匹配的紀錄，則該紀錄不會被加到最後的結果集中 OUTER JOIN: 對於outer join的兩張表，即使驅動表中的紀錄在被驅動表中沒有匹配的紀錄，也仍需要加入到結果集  有時候我們不想把驅動表的全部紀錄都加入到最後的結果集怎辦?  把過濾條件分成兩種: ON跟WHERE  ON: 如果無法在被驅動表中找到匹配ON子句中過濾條件的紀錄，那麼該驅動表紀錄仍會被加入到結果集中，對應的欄位會使用NULL填充 WHERE: 凡不符合WHERE子句中過濾條件的紀錄一律不會被加入到最後的結果集中        加速的方式  Join Buffer: 執行Join前申請一塊固定大小的memory，先把驅動表結果集中的紀錄裝一部份到Join buffer中，然後開始掃描被驅動表，每一筆被驅動表的紀錄一次性與join buffer中的多筆驅動表紀錄進行匹配 Index  子查詢最佳化  按照返回的結果集區分子查詢種類:  純量子查詢: 只返回單一值的子查詢，例如SELECT (SELECT m1 FROM t1 LIMIT 1) 行子查詢: 返回一筆紀錄的子查詢 (返回一個row) 列子查詢: 返回一個column的子查詢 表子查詢: 子查詢既包含多筆row又包含很多個列   按與外層查詢的關係來區分:  不相關子查詢: 子查詢可以單獨運行出結果 相關子查詢: 子查詢的執行需要依賴於外層查詢的值    IN子查詢最佳化 對不相關的IN子查詢來說，如果子查詢結果集中的紀錄比數很少，其實可以把子查詢和外層查詢分別看成兩個單獨的單表查詢。但是如果子查詢的結果集太多，會導致兩個問題:\n 結果集太多，記憶體中放不下 對外層查詢來說，子查詢的結果集太多，則表示IN子句中的參數特別多，將會導致:  無法有效使用索引，只能對外層查詢進行全表掃描 對外層查詢執行權表掃描時，如果IN子句中的參數太多，會導致在檢測一筆紀錄的IN運算式是否為TRUE時花費太多的時間  衍生觀念: IN vs Exists 結論: 外(表)資料大內(表)資料小=IN，外(表)資料小內(表)資量大=EXISTS     一般而言，子查詢的結果集不致於大的離譜，所以預設會為它建立based-on memory儲存引擎(storage engine)的臨時表，並且會為該表建立hash index  如果子查詢的結果集太大，超過了系統變數tmp_table_size或max_heap_table_size，臨時表會轉成用based-on disk的storage engine來保存結果集中的紀錄，index類型也會變成B+ tree index    時間關係來不及補上完整筆記，在此先記錄幾個名詞:\n 物化(materialize): 將子查詢結果集中的紀錄保存到臨時表的過程 子查詢轉join (semi-join)  直接轉會出現問題: join會導致結果集出現duplicate資料 (參考page14-22)   延遲物化 半連結查詢策略  Table pullout duplicate weedout looseScan semi-join materialization firstMatch   如果IN子查詢不能轉為半連接查詢，最佳化工具從兩種策略找出成本低的方式執行子查詢  先將子查詢物化，再執行查詢 執行IN到EXISTS的轉換    InnoDB Buffer pool 什麼是Buffer pool 為了快取disk中的page，InnoDB工程師在MySQL server啟動時就向作業系統要了一片連續的記憶體，這片記憶體就叫Buffer pool\nBuffer pool內部組成  控制區塊: 為了管理緩衝頁，innoDB工程師為每個緩衝頁都建立了一些控制資訊，包含該頁所屬的表格空間編號、頁號、緩衝頁在Buffer pool中的位址、linkedlist節點資訊等等 緩衝頁: Buffer pool對應的記憶體被劃分為許多page，這些page和innoDB表格空間用的大小一樣都是16KB  系統會以表格空間編號+頁號當成key，緩衝頁控制區塊的位址就是對應的value來找到該緩衝頁\nBuffer pool instance示意圖  free list: 紀錄Buffer pool中的哪些page是可用的(把所有空閒的緩衝頁對應的控制區塊當成一個節點放到此linked list中，所以這是一個由控制區塊組成的Linked list) flush list: 儲存髒頁的linked list，凡是被修改過的緩衝頁對應的控制區塊都會被當成一個節點被加入此linkedlist (不可能每次都把所有的緩衝頁刷新到磁碟中，效能會很差) LRU list: Buffer pool滿的時候，要踢掉一些緩衝頁，透過LRU紀錄哪些緩衝頁很少被使用，優先踢掉那些緩衝頁對應的控制區塊  如果該頁不在Buffer pool中，在把該頁從disk載入到Buffer pool時，把該緩衝頁對應的控制區塊作為節點塞到LRU的頭部 如果該頁本來就在Buffer pool中(本就被已經被載入)，則直接把該頁對應的控制區塊移動到LRU的頭部    :::info LRU可能遇到的問題\n InnoDB有預先讀取(read ahead)機制，若預讀的資料都沒有用到的話會把LRU上原本的資料都淘汰掉，大幅降低Buffer pool命中率 進行全表掃描會把原本在Buffer pool存取頻率高的頁給淘汰掉   為解決上述問題，InnoDB工程師把LRU按比例(63/37)比例分為young區(熱資料)以及old區(冷資料)，搭配以下解法:    初次載入Buffer pool的緩衝頁會放到old區的頭部，如此一來預讀到Buffer pool但後續沒用到的頁就會逐漸從old區被逐出，同時不影響young區中使用頻率高的緩衝頁\n  old區的資料載入後只要被讀取就會被加到young區，InnoDB工程師定義一個時間間格，只要後續access與第一次access的時間在某個時間間隔內，該頁就不會從old區被移到young區頭部\n  進一步優化: 如果每存取一個緩衝頁就要把它移動到young區頭部，負擔太大(畢竟young區是熱資料，常被存取)，因此InnoDB工程師提出一些優化策略，例如存取的緩衝頁位於young區1/4區域的後面時，才會被移到LRU的頭部 :::\n  刷新髒頁到磁碟 刷新的方式有兩種:\n BUF_FLUSH_LRU: 從LRU上的冷資料刷一部份頁到磁碟 BUF_FLUSH_LIST: 從Flush上刷一部份頁到磁碟  多個Buffer Pool實例 在Buffer Pool特別大且多個thread存取量高的情況下，單一Buffer Pool可能會影響請求的處理速度。(在multithread環境下存取Buffer Pool的各個list都需要加鎖處理) 所以在Buffer Pool特別大的時候可以把他們拆分成許多小的Buffer Pool，每個Buffer Pool都稱為一個實例\ninnodb_buffer_pool_chunk_size 每次調整Buffer Pool大小都需要向OS要一塊連續的記憶體空間，然後將舊Buffer Pool上的內容複製到新的空間，很耗時。解法是以chunk為單位向OS申請空間，如此才能達到Server運行期間調整Buffer Pool大小卻不影響太多效能\nRedo log Mini-Trancsaction(MTR)  樂觀插入(insert) 悲觀插入(insert): insert資料時需要進行page split  向某個B+樹插入一筆record的過程必須是原子的，不能插了一半就停止。 例如在悲觀插入中，新的page已經分配下去，資料也從原本的page複製過去了，新的record也插入到Page中了，但是內節點(internal node，紀錄頁號和頁的紀錄中最小的主鍵值)卻沒有插入一筆目錄項的record，這樣這個插入過程就是不完整的，會形成一顆不正確的B+樹\nredo log block  MTR產生的redo log檔會放在大小為512 bytes的page中，這個page被稱為block 寫入redo記錄檔的時候不會直接寫到磁碟中，實際上在server啟動時跟buffer pool一樣會向OS要一大塊redo log buffer的連續記憶體空間  此記憶體空間被劃分成許多連續的redo log block   redo記錄檔寫入磁碟時機  log buffer空間不足時: 佔log buffer總容量達50%時會寫入 交易提交時: 雖然提交時可以不把修改過的buffer pool page立刻刷新到磁碟，但為了保證持久性，必續把page修改時對應的redo記錄檔刷新到磁碟 正常關閉SERVER時 做checkpoint時 後臺其實有個thread daemon大概以每秒1次的頻率將log buffer刷新到磁碟    MVCC MVCC（Multi-Version Concurrency Control，多版本並發控制）是MySQL中用於實現事務並發的機制。MVCC使用ReadView和Undo Log來實現並發控制：通過為每個事務更改資料留下紀錄（Undo Log），並用ReadView拍下數據快照來選擇要用哪個版本紀錄，從而避免了讀寫衝突（幻讀）和阻塞（加鎖）。\nUndo Log Undo Log（回滾日誌）是MySQL中用於實現事務的原子性(Atomicity)的一個重要機制，【事務的ACID說明】，它不僅是MVCC重要的功能，也為事務的回滾（Rollback）提供實現：每個事務在執行修改（INSERT、UPDATE、DELETE）資料操作時，都會記錄下回滾需要的資料到Log 中，例：\n  插入一條紀錄時，就把此紀錄的primary key(cluster index)記下，之後回滾只需要將此primary key 對應的資料刪除就好；反之亦然\n  在更新紀錄時，一樣記下舊資料，這樣之後回滾就把舊資料更新回去就好 不同的操作要記錄下的內容不盡相同，更詳細的內容參考官方文件。但不論是哪一種操作紀錄的undo log都一定有一個roll_pointer跟trx_id。\n  roll_pointer：指向上一版本的undo log，所有roll_pointer連結所形成表就叫做版本鏈，透過此鏈可以找到這一條資料的所有歷史紀錄\n  trx_id：記錄此更改是哪一個事務造成的。InnoDB 會記錄每一個事務的ID，叫作transaction id，事物開始時會由InnoDB 指派，此ID 是嚴格遞增的，因此ID數字大的事務一定比數字小的事務晚開始\n  交易併發執行的時候可能產生的一致性問題: (trx=transacitons)\n 贓寫入(dirty write): 某個trx修改了另一個未提交(uncommitted)的資料 Dirty Read: 讀到了未提交trx修改過的資料  當一個trx允許讀取另外一個trx修改但未提交的資料時可能發生   Non-Repeatable Read: 修改了未提交交易讀到的資料  當執行SELECT操作時沒有獲得讀鎖或者SELECT操作執行完後馬上釋放了讀鎖   Phantom Read: 在trx中執行兩次的一樣的query時，the collection of rows returned by the second query is different from the first.   :::info Simple examples:\n User A runs the same query twice. In between, User B runs a transaction and commits. Non-repeatable read: The A row that user A has queried has a different value the second time. Phantom read: All the rows in the query have the same value before and after, but different rows are being selected (because B has deleted or inserted some). Example: select sum(x) from table; will return a different result even if none of the affected rows themselves have been updated, if rows have been added or deleted.:::      鎖 寫-寫的狀態一定要加鎖 讀寫/寫讀則依需求決定用MVCC的方式還是加鎖(加鎖會影響效能)。一般情境下當然傾向採用MVCC，除非特殊的業務場景要求必須用加鎖的方式執行(例如銀行交易)\nConsistent Read Locking Read (鎖定讀取) 鎖的種類\n Shared Lock: Exclusive Lock  SQL的4種隔離等級  Read Uncommitted Read Committed Repeatable Read Serializable  Advanced MySQL and MariaDB MYSQL handle scaling的方式:  Traditional threading   one thread per connection (每個connection有自己的resource去run quires)  可能會造成使用過多的資源 good for disk-bound workloads(有多個thread可以避免等IO) Bad for CPU-bound workloads(太多thread，context switch負擔大)   適合connection不多的情況 (works for a small number of connections)  disk access是重要factor的時候 (適合需要一直等I/O的情況)    比較好使用資源的方式是透過thread pooing    有限的thread數\n  適合CPU-bound的quries (不用一直context switch)\n 但不適合never wait的long-running quries(會delay其他quries)    thread pool通常會在quries上加上delays (因為要等thread allocated to them)\n  不適合bursty的情境(thread數量有限沒辦法處理大量需求)\n  too many thread:\n too much CPU overhead too much memory    too few thread:\n not using available resources queries may run more slowly    建議thread數量和CPU數一樣(one thread per CPU)\n  MySQL 和 MariaDB都implement server-based thread pooling  MySQL只有企業版有 (作為plugin) MariaDB則是內建的  設定thread pooling  thread_pool_size: 設定thread groups的數量  每個connection都會被assigned到一個thread group (用round-robin的方式) thread_pool_size預設16，最大4096 在MYSQL中是靜態的(須重啟才會吃到設定); MariaDB中則是動態的(不須restart)   當thread pooling work well, 除了busrtiness太常發生  可以透過設定thread pool中的threads的數量 thread+pool_max_threads預設是500 在mySQL和MariaDB都是dynamics(不須restart)   每個groups都有一個listener thread去聽要連進來的connections (incoming connections)  當connection被assign到一個group的時候，如果groups內沒有其他quries等著要執行，那個listener就會去執行該query  也就是當thread group是空的時候, listener thread會開始執行query並且暫時unavailable 因為透過round-robin去assign connection的原因，所以影響還好 (該thread groups暫時不會收到其他的connection) 如果query花太多時間，thread group會創建新的listener thread去聽quries 過了多久才要創新的listener thread可以透過thread_pool_stall_limit變數去設定     thread_pool_stall_limit預設是60ms,最大是6秒  當達到thread_pool_max_threads上限時，listener thread不會被創建 :::info mysql5.6之前處理客戶端連接的方式會觸發mysql 新建一個thread來處理新的connection，新建的thread會處理該connection所發送的所有 SQL 請求，即 one-thread-per-connection 的方式，其創建connection的stack為：    0 mysql_execute_command 1 0x0000000000936f40 in mysql_parse 2 0x0000000000920664 in dispatch_command 3 0x000000000091e951 in do_command 4 0x00000000008c2cd4 in do_handle_one_connection 5 0x00000000008c2442 in handle_one_connection 6 0x0000003562e07851 in start_thread () from /lib64/libpthread.so.0 7 0x0000003562ae767d in clone () from /lib64/libc.so.6 優點及存在的問題 在連接數較小的情況下可以很快的響應客戶端的請求，但當連接數非常大時會創建很多線程，這樣會引起以下問題：\n 過多線程之間的切換會加重系統的負載，造成系統資源緊張且響應不及時； 頻繁的進行線程的創建及銷毀以及線程間同時無序的竟爭系統資源加重了系統的負載。  thread_pool正是為了解決以上問題而產生的；\nthread_pool(線程池)，是指mysql 創建若干工作線程來共同處理所有連接的用戶請求，用戶的請求的方式不再是 『one thread per connection』，而是多個線程共同接收並處理多個連接的請求\nhttps://kknews.cc/zh-tw/code/344yexa.html :::\nGTID (global transaction identifiers) GTIDs in MySQL  two-part GTID  Source id: 用來辨識transaction從哪個server來的(server UUID形式) Transaction id: (sequence number)   GTID範例:  假設有一個server id b8617fde-2810-11e4-bf16-98fe9446b48a 對應的GTID為b8617fde-2810-11e4-bf16-98fe9446b48a:1-147 這筆GTID的意思是該server has applied transactions 1到147   MySQL GTID Replication Setup  需要設定下列4個static參數  [mysqld] gtid-mode=ON log-bin log-slave-updates # 為了 to log all transactions enforce-gtid-consistency # disable statements that are not GTID-safe    MySQL GTID Slave Setup   CHANGE MASTER TO MASTER_HOST='db.example.com', MASTER_PORT=3306, MASTER_USER='\u0026lt;your user name\u0026gt;', MASTER_PASSWORD='\u0026lt;your pwd\u0026gt;', MASTER_AUTO_POSITION=1; START SLAVE;    replicate的步驟會先compares to GTIDs done接著才applies remaining GTIDs  GTIDs in MariaDB   Still positional\n matser is considered a source, replication is treated as a stream    three-part GTID\n Domain id: 用來辨識replication stream (integer type) Server id: 用來辨識originating server Sequence number: same as MySQL    GTID範例:\n 看起來會像7-943-20  7是domain id, 辨認這是哪個replication stream 943是server id, 用來告訴mariaDB這個statement從哪個server來的 20其實就是transaction sequence id，用來告訴mariaDB這個slave現在在replication stream的哪個位置      New Slave\n No range comparison like in MySQL GTIDs No backfill of any missing transactions    MariaDB GTIDs特性\n less automatic than MySQL GTIDs (因為maria depends on fixed position)  也因此maria的slave可以支援more than one master(此特性也被稱為multisource replication)   預設啟動，不需要額外config to enable GTIDs in MariaDB    MariaDB GTID Replication Setup\n 不需要額外設定，除非使用more than 1 write sources，這種情況的話就要設定domain id  [mysqld] domain-id=12 # 此id為舉例用     MariaDB GTID Slave Setup\n  CHANGE MASTER TO MASTER_HOST='db.example.com', MASTER_PORT=3306, MASTER_USER='\u0026lt;your user name\u0026gt;', MASTER_PASSWORD='\u0026lt;your pwd\u0026gt;', MASTER_USE_GTID=current_pos; START SLAVE;     Multisource Replication  MariaDB only Slave間的互動更複雜(complicated)  MariaDB架構介紹 在oracle的世界中，有所謂的tablespace的概念， 原因是以前單一檔案有大小限制，為了讓table突破容量限制，引進了tablespace的概念 透過邏輯的方式讓table對應到多個datafile\n但是比較新的database通常都是1張table對應1個data file，也就沒有tablespace的概念了 (檔案不像以前有大小限制了)\n\u0026lt;Oracle\u0026gt; Server Database Tablespace-DataFile Table\n\u0026lt;MariaDB\u0026gt; Server Database Table-DataFile\nMySQL實戰: 指令解說 這個section記錄了常用的指令\nGRANT GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user_or_role [, user_or_role] ... [WITH GRANT OPTION] [AS user [WITH ROLE DEFAULT | NONE | ALL | ALL EXCEPT role [, role ] ... | role [, role ] ... ] ] } GRANT PROXY ON user_or_role TO user_or_role [, user_or_role] ... [WITH GRANT OPTION] GRANT role [, role] ... TO user_or_role [, user_or_role] ... [WITH ADMIN OPTION] object_type: { TABLE | FUNCTION | PROCEDURE } priv_level: { * | *.* | db_name.* | db_name.tbl_name | tbl_name | db_name.routine_name } user_or_role: { user (see Section 6.2.4, “Specifying Account Names”) | role (see Section 6.2.5, “Specifying Role Names”) } Account Names (帳號名稱) A user value in a GRANT statement indicates a MySQL account to which the statement applies. To accommodate granting rights to users from arbitrary hosts, MySQL supports specifying the user value in the form \u0026lsquo;user_name\u0026rsquo;@\u0026lsquo;host_name\u0026rsquo;.\nYou can specify wildcards in the host name. For example, \u0026lsquo;user_name\u0026rsquo;@'%.example.com\u0026rsquo; applies to user_name for any host in the example.com domain, and \u0026lsquo;user_name\u0026rsquo;@\u0026lsquo;198.51.100.%\u0026rsquo; applies to user_name for any host in the 198.51.100 class C subnet.\nThe simple form \u0026lsquo;user_name\u0026rsquo; is a synonym for \u0026lsquo;user_name\u0026rsquo;@'%'.\nMySQL的user name不支援 wildcards. 至於匿名使用者(anonymous user), 在指定account的時候給空值即可(specify an account with an empty user name with the GRANT statement):\nGRANT ALL ON test.* TO ''@'localhost' ...;\n匿名使用者\nRDB vs NoSQL:   NoSQL雖然有優勢，但是這些優勢是需要付出代價的，例如絕大部分只支持key-value的操作，不能使用join的功能 資料安全性跟穩定性的議題，適合業務複雜度不高的應用 至於handlersocket，應用上簡單快速的操作可以透過handlersocket實現，複雜的操作，還是會透過傳統MySQL的方式實現 MariaDB Knowledge Base - Handle socket\n  資料機敏性高的時候是用RDB，NoSQL常會有資料不一致問題 (data inconsistency)\n  ","permalink":"https://kuanyuce.github.io/blog/mysql/","tags":["自學筆記","Tool"],"title":"MySQL隨筆"},{"categories":["Programming Language"],"contents":"Thread 要執行Thread程式，最簡單的方式就是定義一個class繼承Runnable interface，並呼叫start()開始執行:\npackage cc.openhome; public class Hare implements Runnable { private boolean[] flags = {true, false}; private int totalStep; private int step; public Hare(int totalStep) { this.totalStep = totalStep; } @Override public void run() { while(step \u0026lt; totalStep) { var isHareSleep = flags[((int) (Math.random() * 10)) % 2]; if(isHareSleep) { System.out.println(\u0026quot;兔子睡著了zzzz\u0026quot;); } else { step += 2; System.out.printf(\u0026quot;兔子跑了 %d 步...%n\u0026quot;, step); } } } } package cc.openhome; public class TortoiseHareRace2 { public static void main(String[] args) { var tortoise = new Tortoise(10); var hare = new Hare(10); var tortoiseThread = new Thread(tortoise); var hareThread = new Thread(hare); tortoiseThread.start(); hareThread.start(); } } 如果要跑多執行緒，可以將流程定義在Runnable的run()方法，或者繼承Thread類別，並重寫override run()方法。實作Runnable比較有彈性，因為類別還可以繼承其他類別。但若繼承了Thread，那該類別就是一種Thread了。\n// Runnable實例也可以透過Lambda表示來實作，例如: var someThread = new Thread() { public void run() { // implementation } }; // 可以改成 var someThread = new Thread(() -\u0026gt; { //implementation }); synchronized / volatile 每個物件都會有個intrinsic lock (內部鎖，或稱為monitor lock)。被標示為synchronized的區塊會被監控，任何thread要執行synchronized區塊都必須先取得物件的內部鎖。 若在方法前加上synchronized，執行方法必須取得該實例的內部鎖\n基於效率，thread通常會將變數的值cache到自己的記憶體空間中，完成操作再對變數進行更新。問題在快取的時機不一定，若有多個thread存取某個變數，有可能發生變數已經更新，但某些thread還在使用cache值。除了加上synchronized限制存取外，可以在變數上宣告volatile，被標示為volatile的變數，不允許thread快取。\n等待與通知 wait()/notify()/notifyAll()是object定義的方法，用來控制thread釋放物件的內部鎖或通知thead要參加內部鎖的競爭\n執行synchronized的期間，如果呼叫該物件的wait()方法，thread會釋放內部鎖並進入物件的wait set而變成blocked狀態，此時其他thread就可以競爭內部鎖。放在wait set的thread不會參與cpu schedule。wait()可以指定等待時間，時間到thread會再度加入排班；然而若不指定或指定時間為0，thread會持續等待，直到被中斷(interrupt())或通知(notify())可以參加排班\n並行API 使用Thread建立多執行緒程式，必須親自處理synchronized、物件鎖定、wait()/notify()/notifyAll()等細節，如果需要的是thread pool、讀寫鎖等高階操作，可以使用java.util.concurrent套件建立更穩固的併行應用程式\nExecutor Thread的建立與系統資源有關，如何建立thread、是否重複使用thread、何時銷毀thread、runnable何時排定給thread執行都是複雜的議題。因此java定義了java.util.concurrent.Executor的介面，目的在將Runnable的指定與執行分離，executor介面只定義了一個execute()方法\npackage java.util.concurrent public interface Executor { void execute(Runnable command); } Lambda可以當成runnable!\n範例:\n// Pages.java package cc.openhome; import java.net.URI; import java.net.http.HttpClient; import java.net.http.HttpRequest; import java.net.http.HttpResponse.BodyHandlers; import java.util.concurrent.*; import java.io.*; public class Pages { private String[] urls; private String[] fileNames; private Executor executor; public Pages(String[] urls, String[] fileNames, Executor executor) { this.urls = urls; this.fileNames = fileNames; this.executor = executor; } public void download() { for(var i = 0; i \u0026lt; urls.length; i++) { var url = urls[i]; var fileName = fileNames[i]; executor.execute(() -\u0026gt; { try { dump(openStream(url), new FileOutputStream(fileName)); } catch (Exception ex) { throw new RuntimeException(ex); } }); } } private InputStream openStream(String uri) throws Exception { return HttpClient .newHttpClient() .send( HttpRequest.newBuilder(URI.create(uri)).build(), BodyHandlers.ofInputStream() ) .body(); } private void dump(InputStream src, OutputStream dest) throws IOException { try(src; dest) { var data = new byte[1024]; var length = 0; while((length = src.read(data)) != -1) { dest.write(data, 0, length); } } } } 簡單定義一個Executor\n// 這個executor只會逐一下載 package cc.openhome; import java.util.concurrent.Executor; public class DirectExecutor implements Executor { public void execute(Runnable r) { r.run(); } } 定義一個ThreadPerTaskExecutor\n// 這個executor會針對每個網頁啟動一個Thread進行下載 package cc.openhome; import java.util.concurrent.Executor; public class ThreadPerTaskExecutor implements Executor { public void execute(Runnable r) { new Thread(r).start(); } } 也可以搭配threadpool使用\npackage cc.openhome; import java.util.concurrent.Executors; public class Download3 { public static void main(String[] args) { String[] urls = { \u0026quot;https://openhome.cc/Gossip/Encoding/\u0026quot;, \u0026quot;https://openhome.cc/Gossip/Scala/\u0026quot;, \u0026quot;https://openhome.cc/Gossip/JavaScript/\u0026quot;, \u0026quot;https://openhome.cc/Gossip/Python/\u0026quot; }; String[] fileNames = { \u0026quot;Encoding.html\u0026quot;, \u0026quot;Scala.html\u0026quot;, \u0026quot;JavaScript.html\u0026quot;, \u0026quot;Python.html\u0026quot; }; var executorService = Executors.newCachedThreadPool(); new Pages(urls, fileNames, executorService).download(); executorService.shutdown(); } } ","permalink":"https://kuanyuce.github.io/blog/javathread/","tags":["Java"],"title":"Java Thread"},{"categories":null,"contents":"Java-GQL-POC Sample Data  sample/lot_proc_time.json sample/wafer_main.json  Run mvn spring-boot:run Sample Query query { getWaferByLot(lot_id: [\u0026#34;EE\u0026#34;, \u0026#34;EE\u0026#34;], tz: \u0026#34;GMT+8\u0026#34;) { lot_id wafer_id eqp_id step_seq lot_proc_time { lot_id op_start proc_start proc_end op_end load_status } } } ","permalink":"https://kuanyuce.github.io/blog/lru/","tags":null,"title":""}]